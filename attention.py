import math
import torch
class SelfAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        pass
    def forward(self, attention_mask):
        pass